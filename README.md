# oktoblas
High-performance BLAS library with Tensor Core acceleration. 125% PyTorch FP16 GEMM, 3x faster Fused Attention. 100% independent, no cuBLAS.
